import { Callout } from "nextra/components";

# Unit 4

## <mark> 1) Write a note on peephole optimization.or Explain Peephole Optimization. </mark>

### Peephole Optimization

**Peephole optimization** is a **local code optimization technique** applied to a small sequence of instructions (a "peephole") in the generated code. It replaces inefficient patterns with equivalent but more efficient code, improving performance and reducing code size. This technique operates on **low-level intermediate code** or **target machine code** and is often machine-dependent[1][10].

#### **Key Concepts**

1. **Sliding Window (Peephole)**:

   - Examines a fixed-size window of consecutive instructions (e.g., 2–5 instructions).
   - The window "slides" through the code to identify optimization opportunities.

2. **Common Optimizations**:

   - **Redundant Instruction Elimination**:
     ```assembly
     MOV R1, R2
     MOV R2, R1   → Eliminated (redundant copy).
     ```
   - **Algebraic Simplification**:
     ```assembly
     ADD R1, 0       → Removed (no effect).
     MUL R2, 2       → Replaced with `SHL R2, 1` (shift-left is faster).
     ```
   - **Constant Folding**:
     ```assembly
     MOV R1, 5*3+2   → Replaced with `MOV R1, 17`.
     ```
   - **Dead Code Elimination**:
     ```assembly
     JMP L1
     MOV R1, R2      → Removed (unreachable code).
     ```
   - **Strength Reduction**:
     ```assembly
     MUL R1, 4       → Replaced with `SHL R1, 2` (shift is faster than multiply).
     ```

3. **Control-Flow Optimizations**:
   - **Jump Chaining**:
     ```assembly
     JMP L1
     ...
     L1: JMP L2      → Replaced with direct `JMP L2`.
     ```

---

#### **Benefits**

- **Performance Improvement**: Faster execution by replacing slow instructions.
- **Code Size Reduction**: Eliminates redundant or unnecessary instructions.
- **Machine-Specific Tuning**: Exploits target architecture features (e.g., specialized instructions).

---

#### **Implementation**

- Applied **after** global optimizations (e.g., data-flow analysis).
- Uses **pattern-matching** to identify replaceable instruction sequences.
- Example from the textbook:
  - **Figure 10.2** illustrates how a peephole optimizer scans code and replaces patterns like redundant loads/stores[10].

---

#### **Example**

**Original Code**:

```assembly
MOV R1, a
MOV a, R1
ADD R2, 0
```

**After Peephole Optimization**:

```assembly
MOV R1, a    ; Redundant "MOV a, R1" removed
             ; "ADD R2, 0" eliminated
```

---

**In Short**: Peephole optimization refines code at the instruction level, focusing on small, localized improvements to enhance efficiency without altering program semantics[10].

## <mark> 2) Explain code generator design issues. or Explain various issues in design of code generator. </mark>

When designing a code generator, several critical issues need to be addressed to ensure efficient and correct translation of intermediate code into machine-executable code. These issues include[cite: 1]:

- **Input Intermediate Representation (IR)**: The code generator receives the Intermediate Representation (IR) from the semantic analysis phase, along with information from the Symbol Table. The Symbol Table contains vital type information for variables and expressions, which is crucial for code generation. Common IR forms include Reverse Polish Notation (RPN), 4-tuple (three-address code), and Abstract Syntax Tree (AST)[cite: 16].

- **Nature of Target Language**: The characteristics of the target machine's instruction set significantly impact the complexity and effectiveness of the generated code. Key considerations involve:

  - **Processor Architecture**: Whether the target is a Complex Instruction Set Computer (CISC) or a Reduced Instruction Set Computer (RISC) influences instruction selection and optimization strategies. CISC architectures often have fewer CPU registers, variable-length instructions, and diverse addressing modes, while RISC architectures feature numerous CPU registers, fixed-length instructions, and fewer addressing modes[cite: 16].
  - **Instruction Orthogonality**: A highly orthogonal instruction set (where most instructions can use any register and data type with consistent semantics) simplifies target code optimization[cite: 16].
  - **Instruction Set Design**: Instruction sets are often designed with common code idioms in mind. Understanding these idioms can aid in selecting appropriate instructions for specific computation patterns[cite: 17].

- **Data Structures**: The code generator must effectively handle various data structures, such as vectors, arrays, and records (like `struct` in C). This involves determining their memory layout and generating the necessary run-time code for accessing their elements. For instance, elements of vectors and arrays are typically stored in contiguous memory locations[cite: 19].

- **Control Constructs**: Generating efficient code for control flow constructs (e.g., loops, conditional statements) is another design concern. Optimizations, such as moving common calculations outside of loops, can significantly improve performance[cite: 20].

- **Procedures and Function Calls**: The design of activation records and the conventions for function calls are crucial for correct program execution and interoperability. Adhering to conventions, such as "caller cleans the stack," can allow for variable numbers of arguments and compatibility with existing compiler conventions, like those used by GCC C[cite: 21, 22].

- **Target Operating Environment**: The specifics of the operating environment where the generated code will run must also be considered, as it influences aspects like system calls and memory management[cite: 6].

- **Code Optimization**: While code optimization is a distinct phase, it is also a significant design concern for the code generator. This includes machine-dependent optimizations like register allocation (assigning program variables to CPU registers for faster access) and instruction reordering to take advantage of parallel execution capabilities of modern processors[cite: 6, 17, 23, 24].

## <mark> 3) Give Data Flow Properties. </mark>

Data flow properties are crucial for global data-flow analysis in compilers, which aims to gather information about the possible set of values computed at various points in a program. Key properties include:

- **Defined Expression**: An expression is considered "defined" at a specific point in the program if its value is computed there[cite: 23, 24, 25].
- **Killed Expression**: An expression is "killed" by a redefinition of any of its argument variables[cite: 23, 24, 25].
- **Available Expression**: An expression is "available" at a point `p` in the program's control flow graph (represented as a directed graph G) if every path leading to `p` includes a prior definition of that expression, and that definition has not been subsequently "killed" along that path[cite: 23, 24, 25].

These properties are fundamental to optimizing compilers, enabling techniques like common sub-expression elimination and dead code removal.

## <mark> 4) Explain role of code generator. </mark>

Certainly! Here is a clear and easy-to-remember explanation of the **role of the code generator** in a compiler, as described in your textbook (see Chapter 9, “Code Generation and Machine-dependent Optimization”).

---

### Role of Code Generator

The **code generator** is the phase of a compiler that takes the intermediate code (produced after analysis and optimization) and translates it into the final machine code or assembly language for the target computer.

#### Main Roles:

1. **Translation to Target Code**

   - Converts intermediate representations (like three-address code or abstract syntax trees) into machine instructions that the computer can execute.

2. **Instruction Selection**

   - Chooses the most efficient machine instructions for each operation in the intermediate code.
   - Example: Replacing multiplication by 2 with a faster shift-left instruction, if possible.

3. **Register Allocation**

   - Decides which variables and temporary values will be kept in the CPU’s registers (which are fast) and which will be stored in memory (which is slower).

4. **Address Calculation**

   - Calculates the correct memory addresses for variables, arrays, and pointers, considering the target machine’s addressing modes.

5. **Resource Management**

   - Manages limited resources like CPU registers and memory efficiently.

6. **Preserving Program Semantics**
   - Ensures that the meaning and logic of the original program are preserved in the generated code.

#### Example

Suppose the intermediate code is:

```
t1 = a + b
t2 = t1 * c
```

A code generator for an x86 machine might produce:

```
MOV R1, a
ADD R1, b
MUL R1, c
```

Here, the code generator:

- Selects instructions (`MOV`, `ADD`, `MUL`)
- Uses registers (`R1`)
- Maintains the correct order and meaning

#### In Summary

The **code generator** is responsible for producing efficient, correct, and executable code for the target machine. It bridges the gap between high-level program logic and the low-level instructions that a computer can run.

---

**Tip:** Good code generation makes programs run faster and use less memory!

## <mark> 5) Explain various issues in design of code generator. </mark>

When designing a code generator, several critical issues need to be addressed to ensure efficient and correct translation of intermediate code into machine-executable code. These issues include:

- **Input Intermediate Representation (IR)**: The code generator receives the Intermediate Representation (IR) from the semantic analysis phase, along with information from the Symbol Table. The Symbol Table contains vital type information for variables and expressions, which is crucial for code generation. Common IR forms include Reverse Polish Notation (RPN), 4-tuple (three-address code), and Abstract Syntax Tree (AST).

- **Nature of Target Language**: The characteristics of the target machine's instruction set significantly impact the complexity and effectiveness of the generated code. Key considerations involve:

  - **Processor Architecture**: Whether the target is a Complex Instruction Set Computer (CISC) or a Reduced Instruction Set Computer (RISC) influences instruction selection and optimization strategies. CISC architectures often have fewer CPU registers, variable-length instructions, and diverse addressing modes, while RISC architectures feature numerous CPU registers, fixed-length instructions, and fewer addressing modes.
  - **Instruction Orthogonality**: A highly orthogonal instruction set (where most instructions can use any register and data type with consistent semantics) simplifies target code optimization.
  - **Instruction Set Design**: Instruction sets are often designed with common code idioms in mind. Understanding these idioms can aid in selecting appropriate instructions for specific computation patterns.

- **Data Structures**: The code generator must effectively handle various data structures, such as vectors, arrays, and records (like `struct` in C). This involves determining their memory layout and generating the necessary run-time code for accessing their elements. For instance, elements of vectors and arrays are typically stored in contiguous memory locations.

- **Control Constructs**: Generating efficient code for control flow constructs (e.g., loops, conditional statements) is another design concern. Optimizations, such as moving common calculations outside of loops, can significantly improve performance.

- **Procedures and Function Calls**: The design of activation records and the conventions for function calls are crucial for correct program execution and interoperability. Adhering to conventions, such as "caller cleans the stack," can allow for variable numbers of arguments and compatibility with existing compiler conventions, like those used by GCC C.

- **Target Operating Environment**: The specifics of the operating environment where the generated code will run must also be considered, as it influences aspects like system calls and memory management.

- **Code Optimization**: While code optimization is a distinct phase, it is also a significant design concern for the code generator. This includes machine-dependent optimizations like register allocation (assigning program variables to CPU registers for faster access) and instruction reordering to take advantage of parallel execution capabilities of modern processors.

## <mark> 6) Explain the following parameter passing methods. <br> 1. Call-by-value <br> 2. Call-by-reference <br> 3. Copy-Restore <br> 4. Call-by-Name <br> OR <br> Explain various parameter passing methods. </mark>

Parameter passing methods dictate how arguments (actual parameters) are transferred to and from a called function or subroutine (formal parameters). The choice of method affects how changes made to the formal parameters within the function impact the original actual parameters in the calling code.

Here are the various parameter passing methods, explained in detail:

### 1. Call-by-Value (Pass-by-Value)

- **Mechanism:** When a parameter is passed by value, a **copy** of the actual parameter's value is made and given to the formal parameter in the called function. The formal parameter then behaves as a local variable within the function.
- **Behavior:** Any modifications made to the formal parameter inside the function _do not affect_ the original actual parameter in the calling environment, because the function is operating on a separate copy.
- **Semantics:** In-mode (data flows into the function).
- **Implementation:** Typically implemented by copying the value of the argument onto the function's stack frame.
- **Advantages:**
  - **Safety:** The original actual parameter is protected from unintended side effects within the function. This makes functions more predictable and easier to reason about.
  - **Simplicity:** Conceptually straightforward and easy to understand.
- **Disadvantages:**
  - **Overhead for large data types:** Copying large data structures (like arrays or complex objects) can be inefficient in terms of both time and memory.
  - **Cannot return multiple values directly:** Since changes are local, a function cannot directly modify multiple actual parameters in the caller. To return multiple values, one might need to use return types that encapsulate multiple values (e.g., structs, tuples) or use pointers/references (which technically becomes a form of call-by-reference).
- **Common in:** C, C++ (default for primitive types), Java (all parameters are technically pass-by-value, but for objects, the _reference_ to the object is passed by value), Python (all variables are references, and these references are passed by value).

**Example (C++):**

```cpp
#include <iostream>

void increment(int x) { // x is a copy of 'num'
    x = x + 1;
    std::cout << "Inside function: x = " << x << std::endl;
}

int main() {
    int num = 10;
    std::cout << "Before function call: num = " << num << std::endl;
    increment(num); // 'num' is passed by value
    std::cout << "After function call: num = " << num << std::endl;
    return 0;
}
```

**Output:**

```
Before function call: num = 10
Inside function: x = 11
After function call: num = 10
```

### 2. Call-by-Reference (Pass-by-Reference)

- **Mechanism:** When a parameter is passed by reference, the **memory address (or reference)** of the actual parameter is passed to the formal parameter. The formal parameter then acts as an **alias** for the original actual parameter.
- **Behavior:** Any modifications made to the formal parameter inside the function _directly affect_ the original actual parameter in the calling environment, because both refer to the same memory location.
- **Semantics:** Inout-mode (data flows into and out of the function).
- **Implementation:** The address of the actual parameter is pushed onto the stack. Accessing the formal parameter involves an extra indirection (dereferencing the address).
- **Advantages:**
  - **Efficiency:** No copying of large data structures is required, making it efficient for large parameters.
  - **Can modify original data:** Allows a function to directly change the values of actual parameters, which is useful for functions that need to "return" multiple values or modify input data.
- **Disadvantages:**
  - **Side Effects:** Functions can inadvertently modify variables in the calling scope, making code harder to debug and reason about (unintended side effects).
  - **Aliasing:** If the same variable is passed multiple times by reference, or if a global variable is accessed, it can lead to aliasing issues where changes through one formal parameter affect another.
- **Common in:** C++ (using `&`), C (using pointers explicitly), C# (using `ref` keyword), PHP (default for objects, or explicitly using `&`).

**Example (C++):**

```cpp
#include <iostream>

void increment(int &x) { // x is a reference to 'num'
    x = x + 1;
    std::cout << "Inside function: x = " << x << std::endl;
}

int main() {
    int num = 10;
    std::cout << "Before function call: num = " << num << std::endl;
    increment(num); // 'num' is passed by reference
    std::cout << "After function call: num = " << num << std::endl;
    return 0;
}
```

**Output:**

```
Before function call: num = 10
Inside function: x = 11
After function call: num = 11
```

### 3. Copy-Restore (Pass-by-Value-Result / Pass-by-Copy-In-Copy-Out)

- **Mechanism:** This method combines aspects of call-by-value and call-by-reference.
  1.  At the beginning of the function call (call-in), the value of the actual parameter is **copied** into the formal parameter (like call-by-value). The formal parameter is a local variable.
  2.  The function executes, potentially modifying the local formal parameter.
  3.  At the end of the function call (call-out/restore), the final value of the formal parameter is **copied back** into the original actual parameter.
- **Behavior:** It appears to behave like call-by-reference from the caller's perspective (changes are reflected), but without the direct aliasing during function execution. However, in cases of aliasing, its behavior can differ from true call-by-reference.
- **Semantics:** Inout-mode.
- **Implementation:** Requires extra storage for the formal parameter and two copy operations (one in, one out).
- **Advantages:**
  - **No aliasing side effects _during_ execution:** Unlike call-by-reference, if an actual parameter is aliased (e.g., passed twice as arguments), the intermediate updates during the function's execution don't immediately affect the other aliased parameters. The update happens only at the very end.
  - More efficient than call-by-reference if hardware architecture makes copying faster than continuous indirection.
- **Disadvantages:**
  - **Complex behavior with aliasing:** Can lead to confusing and non-intuitive results if actual parameters are aliased, as the final value depends on the order of copy-back.
  - **Overhead:** Involves two copying operations, which can be inefficient for large data structures.
  - Less common in modern high-level languages, though some languages might use it for certain parameter types (e.g., Ada's `in out` mode for non-scalar types might be implemented this way).
- **Consider a problematic aliasing example:**

  ```
  int x = 1;
  void func(a, b):
      a = 2;
      b = 3;

  call func(x, x);
  ```

  - **Call-by-reference:** `x` would become `3` (assuming `b` is copied back last).
  - **Copy-Restore:** If `a` is copied back first, `x` becomes `2`. Then `b` (whose value is `3`) is copied back, so `x` becomes `3`. If `b` copied back first, `x` becomes `3`, then `a` (whose value is `2`) is copied back, so `x` becomes `2`. The final result depends on the order of copy-back, which is usually implementation-defined.

### 4. Call-by-Name

- **Mechanism:** This is a much less common and more complex parameter passing method, famously used in ALGOL 60. Instead of evaluating the actual argument's value or address at the time of the call, the **textual expression** of the actual argument is substituted directly into the function body wherever the formal parameter appears. This substitution happens _at each use_ of the formal parameter within the function.
- **Behavior:** The actual parameter expression is re-evaluated _every time_ the corresponding formal parameter is accessed. This can lead to surprising side effects, especially with arrays or expressions involving global variables.
- **Semantics:** Inout-mode (often behaves like call-by-reference for simple variables but differs for expressions with side effects).
- **Implementation:** Typically implemented using "thunks" (small, anonymous functions or closures that encapsulate the argument expression and its environment, evaluated on demand). This makes it a form of lazy evaluation.
- **Advantages:**
  - **Powerful and flexible:** Can be used to implement custom control structures (like `while` loops or `if` statements) as regular functions.
  - **Lazy evaluation:** Arguments are only evaluated if and when they are actually used inside the function, which can be efficient if arguments are expensive to compute and not always needed.
- **Disadvantages:**
  - **Confusing and non-intuitive side effects:** The re-evaluation of expressions at each use can lead to unexpected behavior, especially when actual parameters involve variables that are modified within the function. This makes reasoning about correctness extremely difficult.
  - **Performance Overhead:** Repeated evaluation of complex argument expressions can be significantly slower than evaluating them once at the call site.
  - **Hard to implement:** Requires sophisticated compiler support (thunks).
  - **Rarely used in modern languages:** Due to its complexity and unpredictable behavior, it has largely been replaced by other mechanisms or language features (like explicit lazy evaluation constructs or closures).
- **Consider a classic problematic example (Swap function):**

  ```
  int i = 1;
  int arr[] = {10, 20, 30};

  // Assume swap(x, y) swaps values of x and y
  void swap(a, b): // formal parameters a, b
      temp = a;
      a = b;
      b = temp;

  // Call swap with actual parameters i, arr[i]
  call swap(i, arr[i]);
  ```

  - **Expected Behavior (by value/reference):** `i` remains `1`, `arr` becomes `{10, 30, 20}` (swapping `arr[1]` and `arr[2]` if `arr[i]` means `arr[1]` initially).
  - **Call-by-Name Behavior:**
    - `temp = i;` (temp = 1)
    - `i = arr[i];` (Since `i` is passed by name, `arr[i]` is re-evaluated. `i` is currently `1`, so `arr[1]` is `20`. So `i` becomes `20`.)
    - `arr[i] = temp;` (Now `i` is `20`. So `arr[20]` (an out-of-bounds access) is assigned `temp` which is `1`.)
      This leads to a completely different and erroneous result.

### Conclusion

The choice of parameter passing method significantly impacts program semantics, efficiency, and safety.

- **Call-by-value** is the most common and safest for primitives, ensuring isolation.
- **Call-by-reference** is efficient for large data and necessary when a function must modify its arguments, but requires careful handling of side effects.
- **Copy-restore** is a less common compromise, trying to offer reference-like behavior without direct aliasing, but with its own complexities.
- **Call-by-name** is largely a historical curiosity in general-purpose languages due to its confusing behavior and performance implications, though the concept of lazy evaluation lives on in other forms.

Modern languages often choose one or two primary methods (usually call-by-value and call-by-reference/pointer) and provide mechanisms (like `const` references in C++) to help manage their trade-offs.

## <mark> 7) Draw a DAG for expression: `a + a * (b – c) + (b – c) * d`. </mark>

https://www.youtube.com/watch?v=fAjIlKmCuUY&t=235s

> Useful part of this vide o is from 03:55 timestamp to 06:10 timestamp.

> This will open the video from 03:55 timestamp, if you want to learn the topic as well then start from the beginning of the video.

## <mark> 8) Explain following: <br> 1. Basic block <br> 2. Constant folding <br> 3. Handle <br> 4. constant propagation <br> 5. Common subexpression elimination <br> 6. variable propagation <br> 7. Code Movement <br> 8. Strength Reduction <br> 9. Dead Code elimination <br> 10. Busy Expression <br> 11. Live Variables </mark>

Certainly! Here are **simple and clear explanations** for each term, as used in compiler design and optimization:

---

### 1. Basic Block

A **basic block** is a straight-line sequence of statements with:

- No branches (except at the end)
- No entry points (except at the beginning)
  **Example:**

```c
a = b + c;
d = a * 2;
print(d);
```

All three statements form one basic block.

---

### 2. Constant Folding

**Constant folding** is an optimization where constant expressions are evaluated at compile time.
**Example:**  
`x = 3 * 4 + 5;`  
becomes  
`x = 17;`

---

### 3. Handle

A **handle** is a substring in a right-sentential form that matches the right side of a grammar rule and can be reduced during bottom-up parsing.

---

### 4. Constant Propagation

**Constant propagation** replaces variables that have constant values with those values.
**Example:**  
If `a = 10; b = a + 5;`  
then `b = 10 + 5;` becomes `b = 15;`

---

### 5. Common Subexpression Elimination

This optimization finds and removes duplicate calculations.
**Example:**  
`x = (a + b) * c; y = (a + b) * d;`  
Calculate `t = a + b;` once, then use `t` in both places.

---

### 6. Variable Propagation

**Variable propagation** replaces a variable with another variable’s value if it’s known and safe.
**Example:**  
If `b = a; c = b + 2;`  
then `c = a + 2;`

---

### 7. Code Movement

Moving code out of loops or blocks to reduce redundant execution.
**Example:**  
Move `x = 5;` out of a loop if `x` does not change inside the loop.

---

### 8. Strength Reduction

Replacing expensive operations with cheaper ones.
**Example:**  
Replace `x = y * 8;` with `x = y << 3;` (shift left by 3).

---

### 9. Dead Code Elimination

Removing code that never affects the program output.
**Example:**

```c
a = 10;
a = 20; // 'a = 10;' is dead code and can be removed
```

---

### 10. Busy Expression

An expression is **busy** at a point if its value will be used before any of its variables are changed.
**Example:**  
In `a = b + c; d = b + c;`, `b + c` is busy after the first statement.

---

### 11. Live Variables

A variable is **live** at a point if its value will be used later in the program.
**Example:**

```c
a = 5;
b = a + 2; // 'a' is live before this line because its value is used in 'b'
```

---

**In summary:**  
These are important terms and techniques in compiler optimization, helping generate efficient and correct code!

## <mark> 9) Explain code-optimization technique in detail. </mark>

Certainly! Here’s a detailed yet easy-to-understand explanation of **code optimization techniques** as per your textbook (see Chapter 10: Code Optimization):

---

### Code Optimization Techniques

**Code optimization** is a phase in a compiler where the intermediate code is improved to produce faster, smaller, or more efficient machine code—without changing the program’s meaning.

#### Types of Code Optimization

##### 1. **Local Optimization**

- Performed within a single basic block (a straight-line code sequence with no jumps).
- Example: Removing redundant calculations within a basic block.

##### 2. **Global Optimization**

- Performed across basic blocks, analyzing the whole function or program.
- Example: Moving code out of loops, eliminating redundant loads/stores.

---

#### Common Code Optimization Techniques

##### 1. **Constant Folding**

- Evaluate constant expressions at compile time.
- Example: `x = 3 * 4 + 2;` becomes `x = 14;`

##### 2. **Constant Propagation**

- Replace variables that hold constant values with the constant itself.
- Example: If `a = 5; b = a + 2;` then `b = 7;`

##### 3. **Common Subexpression Elimination**

- Avoid repeating calculations by reusing previously computed values.
- Example:
  ```
  t1 = a + b
  t2 = a + b
  ```
  becomes
  ```
  t1 = a + b
  t2 = t1
  ```

##### 4. **Dead Code Elimination**

- Remove code that does not affect the program output.
- Example:
  ```
  a = 10;
  a = 20; // 'a = 10;' is dead and can be removed
  ```

##### 5. **Strength Reduction**

- Replace expensive operations with cheaper ones.
- Example: Replace `x = y * 8;` with `x = y << 3;` (shift left by 3).

##### 6. **Loop Optimization**

- **Loop Invariant Code Motion:** Move calculations that don’t change inside the loop to outside the loop.
- **Loop Unrolling:** Repeat the loop body multiple times to reduce loop overhead.
- **Induction Variable Elimination:** Simplify variables that change in a predictable pattern.

##### 7. **Peephole Optimization**

- Look at a small window ("peephole") of code to find patterns that can be replaced by more efficient ones.
- Example: Replace `ADD R1, 0` with nothing (since adding zero does nothing).

##### 8. **Copy Propagation**

- Replace occurrences of variables that are copies of others with the original variable.
- Example: If `b = a; c = b + 2;` then `c = a + 2;`

##### 9. **Code Movement**

- Move code to a place where it will be executed less often, such as moving loop-invariant code outside loops.

---

#### Benefits of Code Optimization

- **Faster Execution:** Programs run quicker due to fewer or faster instructions.
- **Smaller Code Size:** Less memory is used.
- **Lower Power Consumption:** Especially important for embedded and mobile devices.

---

#### Example

Suppose you have:

```c
for (int i = 0; i < 10; i++) {
    x = y * 0;
    z = a + b;
}
```

After optimization:

- `x = y * 0;` becomes `x = 0;` (constant folding)
- `z = a + b;` can be moved outside the loop if `a` and `b` don’t change (loop invariant code motion)

---

**In summary:**  
Code optimization makes programs more efficient by removing unnecessary work, reusing results, and choosing the best instructions, all while keeping the program’s output unchanged. These techniques are essential for high-performance software!

## <mark> 10) Explain Basic Block and Flow Graph with examples. </mark>

https://www.youtube.com/watch?v=tkSsh91ehUA

## <mark> 11) Discuss the functions of error handler. </mark>

Certainly! Here’s a clear and concise explanation of the **functions of an error handler** in a compiler, as described in your textbook (see Chapter 3.4 for scanner error handling and relevant sections for parser and semantic error handling):

---

### Functions of Error Handler in a Compiler

The **error handler** is a crucial component of a compiler. Its main job is to detect, report, and help recover from errors in the source code during compilation, ensuring the process continues as smoothly as possible.

#### **Main Functions:**

1. **Error Detection**

   - Identifies errors in the source code during different phases (lexical, syntax, semantic, etc.).
   - Example: Detecting an illegal character (`@`) in a variable name.

2. **Error Reporting**

   - Provides clear and helpful messages about the error, including:
     - Type of error (e.g., syntax error, type mismatch)
     - Location (line number, column)
     - Description (what went wrong)
   - Example: `Syntax Error: Missing semicolon at line 5`

3. **Error Recovery**

   - Attempts to continue the compilation process after an error is found, so that more errors can be detected in a single run.
   - Common strategies:
     - **Panic Mode:** Skip input symbols until a safe point is reached.
     - **Phrase Level:** Replace or insert symbols to repair the error.
     - **Error Productions:** Add grammar rules to handle common mistakes.
     - **Global Correction:** Make minimal changes to the source to fix errors.

4. **Error Logging**

   - Keeps a record of all errors found, which can be reviewed by the programmer after compilation.

5. **User Guidance**
   - Suggests possible solutions or corrections to help the programmer fix the error.
   - Example: `Did you mean '=' instead of '=='?`

---

#### **Why is Error Handling Important?**

- Helps programmers quickly find and fix mistakes.
- Prevents the compiler from stopping at the first error, allowing more errors to be found in one compilation.
- Improves the overall development experience.

---

#### **Example**

Suppose the code is:

```c
int x = 5
printf("%d", x);
```

- The error handler will detect the missing semicolon, report it with the line number, and may try to recover so the rest of the code is still checked.

---

**In summary:**  
The error handler in a compiler detects, reports, and helps recover from errors, making the compilation process robust and user-friendly.

## <mark> 12) What is DAG? What are its advantages in the context of optimization? How does it help in eliminating common sub expression. </mark>

Certainly! Here’s an easy-to-understand note on **DAGs (Directed Acyclic Graphs)**, their advantages in optimization, and how they help eliminate common subexpressions, as discussed in your textbook (see Chapter 10, Section 10.2: Value Numbering Scheme and DAGs).

---

### What is a DAG?

A **DAG (Directed Acyclic Graph)** is a data structure used in compiler optimization to represent expressions in a basic block.

- **Directed**: Edges have direction (from parent to child).
- **Acyclic**: No cycles—no path leads back to the same node.
- **Nodes**: Represent operations (like +, \*, -) or operands (variables/constants).
- **Edges**: Show dependencies (which values are used in which operations).

---

### Advantages of DAG in Optimization

1. **Identifies Common Subexpressions**

   - If the same computation appears more than once, the DAG will have only one node for it.
   - This avoids recalculating the same value multiple times.

2. **Eliminates Redundant Calculations**

   - By sharing nodes, the compiler generates code that computes each unique subexpression just once.

3. **Helps in Dead Code Elimination**

   - If a value is computed but not used, its node will not be connected to any output node, making it easy to spot and remove.

4. **Simplifies Code Generation**
   - The DAG structure makes it easier to schedule instructions efficiently.

---

### How DAG Helps in Eliminating Common Subexpressions

Suppose you have the expression:

```
a = b + c
d = b + c
```

- Both `a` and `d` use the same subexpression `b + c`.
- In a DAG, there will be only **one node** for `b + c`, with two outgoing edges: one to `a`, one to `d`.
- The compiler will compute `b + c` once and use its value for both `a` and `d`.

#### Example DAG

For `a = b + c; d = b + c;`:

```
   +
  / \
 b   c
/     \
a      d
```

- The `+` node is shared for both assignments.

---

### In Summary

- **DAGs** are used in compilers to represent expressions in a way that exposes optimization opportunities.
- They help eliminate **common subexpressions** by ensuring each unique calculation is done only once.
- This leads to **faster and smaller code**.

**Tip:** Whenever you see repeated calculations in a basic block, a DAG can help the compiler optimize them away!

## <mark> 13) Explain code scheduling constraints in brief. </mark>

Certainly! Here’s a brief and clear explanation of **code scheduling constraints** as discussed in compiler design (see Chapter 9.10 and Chapter 10 in your textbook):

---

### Code Scheduling Constraints

**Code scheduling** is the process of arranging the order of instructions in the final machine code to improve performance (like speed) while maintaining correctness. However, several constraints must be respected during scheduling:

#### 1. **Data Dependency Constraints**

- **Definition:** Some instructions depend on the results of previous instructions.
- **Example:**
  ```
  t1 = a + b
  t2 = t1 * c
  ```
  Here, `t2` must be scheduled after `t1` because it uses `t1`'s result.

#### 2. **Resource Constraints**

- **Definition:** The target machine has limited resources (registers, functional units, memory ports, etc.).
- **Example:** If only one multiplication unit exists, two multiplications cannot be scheduled at the same time.

#### 3. **Control Dependency Constraints**

- **Definition:** The execution of some instructions depends on the outcome of branch or conditional statements.
- **Example:**
  ```
  if (x > 0)
     y = 5;
  ```
  The assignment to `y` must be scheduled only if the condition is true.

#### 4. **Instruction Latency Constraints**

- **Definition:** Some instructions take multiple cycles to complete, so dependent instructions must wait.
- **Example:** A floating-point division may take several cycles before its result can be used.

#### 5. **Pipeline Hazards**

- **Definition:** Modern CPUs use pipelines; certain instruction sequences can cause stalls if not scheduled carefully (data hazards, structural hazards, control hazards).

#### 6. **Preserving Program Semantics**

- **Definition:** The optimized and scheduled code must produce the same result as the original code.

---

**In summary:**  
Code scheduling must respect dependencies, resource limits, and machine features to ensure correct and efficient execution. The main goal is to maximize parallelism and CPU utilization without changing the program’s intended behavior.

## <mark> 14) Explain panic mode and phrase level error recovery techniques. </mark>

Certainly! Here’s a simple and clear explanation of **panic mode** and **phrase level error recovery techniques**, as described in your textbook (see Chapter 3.4 for error handling in the scanner and Chapter 4.1 for parsing):

---

### 1. Panic Mode Error Recovery

**Panic mode** is a straightforward and widely used error recovery technique in compilers, especially in parsers.

- **How it works:**  
  When the compiler detects an error, it discards (skips) input symbols one by one until it finds a special “synchronizing token” (like a semicolon `;`, closing bracket `}`, or end of statement). Parsing then resumes from this point.
- **Purpose:**  
  To quickly recover from errors and continue parsing the rest of the program, so that more errors can be detected in a single compilation run.

- **Example:**

  ```c
  int x = 10
  printf("%d", x);
  ```

  If the semicolon is missing after `10`, the parser will skip tokens until it finds the next semicolon or closing brace, then continue parsing.

- **Advantages:**

  - Simple to implement.
  - Prevents the parser from getting stuck.
  - Allows detection of multiple errors in one pass.

- **Disadvantages:**
  - May skip over large parts of code, missing some errors or reporting misleading errors.

---

### 2. Phrase Level Error Recovery

**Phrase level error recovery** tries to correct the error immediately at the point where it is detected, using small local changes.

- **How it works:**  
  When an error is found, the compiler tries simple fixes, such as:

  - Inserting a missing token (e.g., a missing semicolon).
  - Deleting an extra or incorrect token.
  - Replacing a token with another.
  - Transposing adjacent tokens.

- **Purpose:**  
  To make minimal changes so parsing can continue smoothly, ideally as if the error never happened.

- **Example:**

  ```c
  int x = 10  // missing semicolon
  ```

  The parser may automatically insert a semicolon after `10` and continue.

- **Advantages:**

  - Can often recover from errors with minimal disruption.
  - More accurate error reporting near the actual mistake.

- **Disadvantages:**
  - May introduce new errors if the automatic fix is not what the programmer intended.

---

#### **In summary:**

- **Panic mode** skips ahead to a safe point after an error.
- **Phrase level** tries to fix the error immediately with small changes.
  Both methods help the compiler continue parsing and report more errors in one run, improving the programmer’s experience.

## <mark> 15) What is global optimization? Name the 2 types of analysis performed for global optimization. </mark>

Certainly! Here’s a simple and direct answer based on your textbook:

---

### What is Global Optimization?

**Global optimization** is a phase in the compiler that improves the intermediate code by analyzing and optimizing across **multiple basic blocks** (not just inside a single block). The goal is to make the entire program or function more efficient, rather than just optimizing small sections.

**Example:**

- Moving invariant code out of loops (so it’s not recalculated every time).
- Removing redundant calculations that occur in different basic blocks.

---

### Two Types of Analysis Performed for Global Optimization

1. **Data Flow Analysis**

   - Tracks how data (values of variables) moves and changes throughout the program.
   - Helps in optimizations like dead code elimination, constant propagation, and live variable analysis.

2. **Control Flow Analysis**
   - Examines the order in which different parts of the program (basic blocks) are executed.
   - Helps in understanding loops, branches, and possible paths through the program.

---

**In summary:**  
Global optimization looks at the whole program (not just one block) and uses **data flow analysis** and **control flow analysis** to make code faster and smaller.

## <mark> 16) Explain the following with example <br> 1) Lexical phase error <br> 2) Syntactic phase error </mark>

Certainly! Here’s a simple explanation of both **lexical phase errors** and **syntactic phase errors** with examples:

---

### 1) Lexical Phase Error

**What is it?**  
A lexical phase error occurs during the scanning (lexical analysis) phase of compilation. The lexical analyzer (scanner) reads the source code and breaks it into valid tokens (like keywords, identifiers, numbers, etc.). If it finds a sequence of characters that does not match any valid token pattern, it reports a lexical error.

**Example:**

```c
int $number = 10;
```

Here, `$number` is not a valid identifier in C (identifiers cannot start with `$`). The scanner will report a lexical error at `$`.

**Error Message Example:**

```
Lexical Error: Invalid character '$' at line 1
```

---

### 2) Syntactic Phase Error

**What is it?**  
A syntactic phase error (syntax error) occurs during parsing (syntax analysis). The parser checks if the sequence of tokens from the scanner follows the grammar rules of the programming language. If the tokens are not in a valid order or structure, a syntax error is reported.

**Example:**

```c
int x = 5
printf("%d", x);
```

Here, the semicolon (`;`) is missing after `int x = 5`. The parser expects a semicolon to end the statement, so it reports a syntax error.

**Error Message Example:**

```
Syntax Error: Missing ';' at line 1
```

---

#### **Summary Table**

| Phase     | Error Name    | Example Code      | What Happens?                |
| --------- | ------------- | ----------------- | ---------------------------- |
| Lexical   | Lexical Error | int $number = 10; | Invalid token detected       |
| Syntactic | Syntax Error  | int x = 5         | Invalid token sequence/order |

---

**In short:**

- **Lexical errors** are about invalid characters or tokens.
- **Syntactic errors** are about invalid token sequences or structure.

## <mark> 17) Explain various techniques involved in loop optimization. </mark>

Certainly! Here’s an easy-to-remember summary of the **main techniques involved in loop optimization**, as described in compiler textbooks:

---

### Loop Optimization Techniques

Loop optimization aims to make loops run faster and more efficiently, since loops often take up most of a program’s execution time. Here are the key techniques:

---

#### 1. **Loop Invariant Code Motion**

- **What:** Moves calculations that give the same result on every loop iteration (invariant code) outside the loop.
- **Example:**
  ```c
  for (i = 0; i < n; i++) {
      x = a + b; // Invariant, can move outside
      arr[i] = x * i;
  }
  ```
  Optimized to:
  ```c
  x = a + b;
  for (i = 0; i < n; i++) {
      arr[i] = x * i;
  }
  ```

---

#### 2. **Strength Reduction**

- **What:** Replaces expensive operations inside loops with cheaper ones.
- **Example:** Replace multiplication with addition.
  ```c
  for (i = 0; i < n; i++) {
      x = i * 4;
  }
  ```
  Optimized to:
  ```c
  x = 0;
  for (i = 0; i < n; i++) {
      // Use x instead of i*4
      x = x + 4;
  }
  ```

---

#### 3. **Induction Variable Elimination**

- **What:** Removes extra variables that change in a regular way (induction variables), reducing unnecessary calculations.
- **Example:**
  ```c
  for (i = 0, j = 0; i < n; i++, j += 2) {
      // use i and j
  }
  ```
  If `j` can be expressed in terms of `i`, we can eliminate `j`.

---

#### 4. **Loop Unrolling**

- **What:** Expands the loop body to decrease the number of iterations and reduce loop overhead.
- **Example:**
  ```c
  for (i = 0; i < 4; i++) {
      sum += arr[i];
  }
  ```
  Unrolled to:
  ```c
  sum += arr[0];
  sum += arr[1];
  sum += arr[2];
  sum += arr[3];
  ```

---

#### 5. **Loop Fusion (Combining)**

- **What:** Combines two loops that iterate over the same range into one, reducing loop overhead.
- **Example:**
  ```c
  for (i = 0; i < n; i++) a[i] = b[i] + 1;
  for (i = 0; i < n; i++) c[i] = a[i] * 2;
  ```
  Fused to:
  ```c
  for (i = 0; i < n; i++) {
      a[i] = b[i] + 1;
      c[i] = a[i] * 2;
  }
  ```

---

#### 6. **Loop Fission (Splitting)**

- **What:** Splits one loop into two to improve cache performance or parallelism.
- **Example:**
  ```c
  for (i = 0; i < n; i++) {
      a[i] = b[i] + 1;
      c[i] = d[i] * 2;
  }
  ```
  Split into:
  ```c
  for (i = 0; i < n; i++) a[i] = b[i] + 1;
  for (i = 0; i < n; i++) c[i] = d[i] * 2;
  ```

---

#### 7. **Loop Interchange**

- **What:** Changes the order of nested loops to improve memory access patterns.
- **Example:**
  ```c
  for (i = 0; i < n; i++)
      for (j = 0; j < m; j++)
          a[j][i] = 0;
  ```
  Interchanged to:
  ```c
  for (j = 0; j < m; j++)
      for (i = 0; i < n; i++)
          a[j][i] = 0;
  ```

---

### In Summary

**Loop optimization techniques** like loop invariant code motion, strength reduction, induction variable elimination, loop unrolling, fusion, fission, and interchange help make loops run faster and use less memory. These optimizations are crucial for high-performance programs, especially when working with large data or repeated calculations.

## <mark> 18) What is code optimization?Why is it needed? Classify code optimization and write benefits of it. </mark>

Certainly! Here’s a clear, easy-to-remember answer based on your textbook and standard compiler concepts:

---

### What is Code Optimization?

**Code optimization** is a phase in the compiler where the intermediate code is improved to make the final machine code run faster, use less memory, or both—**without changing the program’s output**.

---

### Why is Code Optimization Needed?

- **Faster Programs:** Optimized code runs quicker, making software more responsive.
- **Smaller Code Size:** Uses less memory, which is important for embedded or mobile devices.
- **Efficient Resource Use:** Reduces CPU time, power, and other resources.
- **Better User Experience:** Applications perform better and more reliably.

---

### Classification of Code Optimization

**Code optimization** can be classified in two main ways:

#### 1. **Local Optimization**

- Applied within a single basic block (a straight-line code sequence).
- Example: Removing redundant instructions inside a block.

#### 2. **Global Optimization**

- Applied across multiple basic blocks or the entire program.
- Example: Moving loop-invariant code outside the loop, eliminating dead code across blocks.

**Other Classifications:**

- **Machine-Independent Optimization:** Works on intermediate code, not specific to any hardware.
- **Machine-Dependent Optimization:** Tailored for a specific processor or machine architecture.

---

### Benefits of Code Optimization

- **Improved Speed:** Programs execute faster.
- **Reduced Memory Usage:** Less storage and RAM needed.
- **Lower Power Consumption:** Important for battery-powered devices.
- **Efficient Use of Hardware:** Makes the most of available CPU and memory.
- **Enhanced Scalability:** Optimized code can handle larger data and more users.

---

#### **In Short:**

Code optimization makes programs **faster, smaller, and more efficient** by improving the code during compilation, leading to better performance and user satisfaction.

## <mark> 19) Write a note on Global data flow analysis. </mark>

Here’s a simple and clear note on **Global Data Flow Analysis**, following the concepts from your textbook (see Chapter 10.5: Global Data Flow Analysis):

---

### Global Data Flow Analysis

**Global data flow analysis** is a technique used by compilers to collect information about how data (variables and expressions) move and change throughout an entire program or function, not just inside a single basic block.

#### **Purpose**

- To enable powerful optimizations like dead code elimination, constant propagation, and common subexpression elimination.
- To understand where variables get their values and where those values are used or changed.

#### **How It Works**

- The program is divided into **basic blocks** (straight-line code with no jumps).
- These blocks are connected in a **control flow graph (CFG)**, showing possible paths of execution.
- The analysis tracks the flow of data (values of variables) across the entire CFG.
- It answers questions like:
  - Is a variable’s value used before it’s changed?
  - Is an expression computed more than once?
  - Can a variable be replaced by a constant everywhere?

#### **Common Types of Global Data Flow Analysis**

1. **Live Variable Analysis:**  
   Finds out which variables hold values that might be used later.
2. **Available Expression Analysis:**  
   Determines which expressions have already been computed and not changed, so their values can be reused.

#### **Example**

Suppose you have:

```c
a = b + c;
d = a + e;
b = 5;
```

- Global data flow analysis can tell if `b + c` is still valid after `b = 5;` (it’s not), so the compiler knows not to reuse the old value.

#### **Benefits**

- Enables optimizations that make code faster and smaller.
- Removes unnecessary calculations and dead code.
- Helps in register allocation and instruction scheduling.

---

**In summary:**  
Global data flow analysis helps the compiler see the “big picture” of how variables and expressions are used across the whole program, enabling advanced optimizations for better performance.

## <mark> 20) Write a short note on a simple Code Generator. </mark>

Here’s a short and clear note on a **simple code generator** as described in compiler textbooks:

---

### Simple Code Generator

A **simple code generator** is a part of the compiler that translates intermediate code (like three-address code or abstract syntax trees) into target machine code or assembly instructions. Its main job is to produce correct and efficient code that the computer can execute.

#### **How it Works**

- The code generator takes each intermediate instruction and maps it to one or more machine instructions.
- It handles basic tasks like instruction selection, register assignment, and address calculation.
- In a simple code generator, these tasks are done in a straightforward way, often without advanced optimizations.

#### **Key Steps**

1. **Instruction Selection:**  
   Choose the appropriate machine instruction for each operation.
2. **Register Assignment:**  
   Assign variables and temporary results to machine registers or memory locations.
3. **Address Calculation:**  
   Compute addresses for variables, arrays, and pointers.

#### **Example**

Suppose the intermediate code is:

```
t1 = a + b
t2 = t1 * c
```

A simple code generator might produce:

```
LOAD R1, a
ADD  R1, b
MUL  R1, c
STORE t2, R1
```

- Here, `R1` is a register, and each line is a direct translation of the intermediate step.

#### **Features**

- Easy to implement and understand.
- Produces working code, but may not always be the most optimized.
- Good for educational purposes or as a starting point for more advanced compilers.

#### **In Summary**

A simple code generator converts intermediate code into machine code by mapping operations directly to instructions, focusing on correctness and simplicity rather than advanced optimization[1].

## <mark> 21) Write applications of DAG. </mark>

Certainly! Here’s a clear and concise note on the **applications of DAG (Directed Acyclic Graph)**, especially in the context of compilers and optimization, as per your textbook and standard compiler concepts:

---

### Applications of DAG

#### 1. **Optimization of Expressions in Compilers**

- **DAGs** are used to represent expressions within a basic block.
- They help in identifying and eliminating **common subexpressions**, so that repeated calculations are performed only once.
- Example: For the code
  ```
  t1 = a + b
  t2 = a + b
  ```
  Both `t1` and `t2` can share the same node for `a + b` in the DAG.

#### 2. **Dead Code Elimination**

- DAGs make it easy to spot computations whose results are never used (dead code).
- If a node in the DAG has no output edge (not used in any further computation), the corresponding code can be safely removed.

#### 3. **Code Generation**

- DAGs help in generating efficient machine code by determining the optimal order of evaluation and minimizing the number of instructions.
- They also help in deciding which intermediate results can be stored in registers.

#### 4. **Instruction Scheduling**

- By analyzing dependencies in the DAG, compilers can schedule instructions to avoid pipeline stalls and improve CPU utilization.

#### 5. **Expression Simplification**

- DAGs can be used to simplify algebraic expressions by merging equivalent nodes and removing unnecessary operations.

#### 6. **Detection of Value Numbering**

- DAGs help assign value numbers to expressions, making it easier to detect when two expressions compute the same value.

---

#### **Summary Table**

| Application Area                 | How DAG Helps                                |
| -------------------------------- | -------------------------------------------- |
| Common Subexpression Elimination | Shares nodes for repeated expressions        |
| Dead Code Elimination            | Identifies unused computations               |
| Code Generation                  | Guides efficient instruction selection/order |
| Instruction Scheduling           | Shows dependencies for better scheduling     |
| Expression Simplification        | Merges and reduces redundant operations      |
| Value Numbering                  | Detects equivalent computations              |

---

**In short:**  
DAGs are powerful tools in compilers for optimizing code, removing redundancy, and generating efficient machine instructions. They are essential for advanced code optimization techniques!

## <mark> 22) Construct DAG for `a + a * (b- c) + (b – c) * d`. also generate three address code

for the same.

https://www.youtube.com/watch?v=fAjIlKmCuUY
_Same videp as mentioned in Question 7_
`

### **Three Address Code Generation**

Let's assign temporary variables for each operation:

| Step | Operation    | Three Address Code |
| ---- | ------------ | ------------------ |
| 1    | t1 = b - c   | `t1 = b - c`       |
| 2    | t2 = a \* t1 | `t2 = a * t1`      |
| 3    | t3 = a + t2  | `t3 = a + t2`      |
| 4    | t4 = t1 \* d | `t4 = t1 * d`      |
| 5    | t5 = t3 + t4 | `t5 = t3 + t4`     |

**So, the three address code is:**

```
t1 = b - c
t2 = a * t1
t3 = a + t2
t4 = t1 * d
t5 = t3 + t4
```

---

#### **Summary Table**

| Node | Represents        | Used in  |
| ---- | ----------------- | -------- |
| N1   | b - c             | N2, N3   |
| N2   | a \* (b - c)      | N4       |
| N3   | (b - c) \* d      | root (+) |
| N4   | a + [a * (b - c)] | root (+) |

---

**In short:**

- The DAG helps avoid recomputing `(b - c)`.
- The three address code uses temporary variables to store intermediate results, reflecting the DAG structure.

## <mark> 23) Draw syntax tree and DAG for the statement `a = (a * b + c) ^ (b + c) * b + c`. Write three address codes from both.

<Callout type="warning">
  I could not find proper solution. If you have one, please send it. However
  here is what I could manage.
</Callout>

Let's break down the statement:

```
a = (a * b + c) ^ (b + c) * b + c
```

and construct both the **syntax tree** and the **DAG**, then generate the **three-address code** for each.

---

### 1. Syntax Tree

A syntax tree shows every operation and operand, even if repeated.

```
          =
        /   \
      a      +
           /   \
         *      c
       /   \
     ^      b
   /   \
 +      +
/ \    / \
*  c  b   c
/ \
a  b
```

But let's clarify the order (assuming `^` is exponentiation and `*` and `+` have their usual precedence):

- First, compute `(a * b + c)` and `(b + c)`
- Then, compute `((a * b + c) ^ (b + c))`
- Then, multiply that by `b`
- Then, add `c`
- Assign the result to `a`

So, the tree is:

```
        =
      /   \
    a      +
         /   \
       *      c
      / \
     ^   b
    / \
  +    +
 / \  / \
*  c b  c
/ \
a  b
```

---

### 2. DAG (Directed Acyclic Graph)

A DAG merges repeated subexpressions. Here, `(b + c)` appears twice.

- Let `t1 = a * b`
- Let `t2 = b + c`
- Let `t3 = t1 + c`
- Let `t4 = t3 ^ t2`
- Let `t5 = t4 * b`
- Let `t6 = t5 + c`
- Assign `a = t6`

**DAG representation:**

```
      =
    /   \
   a     t6
         |
         +
        / \
      t5   c
      |
      *
     / \
   t4   b
   |
   ^
  / \
t3  t2
|    |
+    +
/ \  / \
t1 c b  c
|
*
/ \
a  b
```

But since `t2 = b + c` and `t3 = t1 + c`, and `t1 = a * b`, the nodes for `b + c` and `c` are shared wherever needed.

---

### 3. Three Address Code

**From Syntax Tree (no sharing):**

```
t1 = a * b
t2 = t1 + c
t3 = b + c
t4 = t2 ^ t3
t5 = t4 * b
t6 = t5 + c
a = t6
```

**From DAG (with sharing):**

- Notice `b + c` is computed once and reused.

```
t1 = a * b
t2 = b + c
t3 = t1 + c
t4 = t3 ^ t2
t5 = t4 * b
t6 = t5 + c
a = t6
```

In this case, the three-address code is the same, but in actual code generation, the value of `b + c` (`t2`) is reused instead of recalculated.

---

### **Summary Table**

| Step | Operation | Three Address Code |
| ---- | --------- | ------------------ |
| 1    | a \* b    | t1 = a \* b        |
| 2    | b + c     | t2 = b + c         |
| 3    | t1 + c    | t3 = t1 + c        |
| 4    | t3 ^ t2   | t4 = t3 ^ t2       |
| 5    | t4 \* b   | t5 = t4 \* b       |
| 6    | t5 + c    | t6 = t5 + c        |
| 7    | a = t6    | a = t6             |

---

**In short:**

- **Syntax tree** shows all operations and operands, even duplicates.
- **DAG** merges repeated subexpressions (here, `b + c`).
- **Three-address code** generated from the DAG avoids redundant calculations by reusing results.
